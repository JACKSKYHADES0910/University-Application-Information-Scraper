# -*- coding: utf-8 -*-
"""
è´å°”æ³•æ–¯ç‰¹å¥³ç‹å¤§å­¦ (Queen's University Belfast) çˆ¬è™«æ¨¡å—
è´Ÿè´£æŠ“å– QUB Postgraduate Taught é¡¹ç›®ä¿¡æ¯
"""

import time
import re
from typing import List, Dict
from urllib.parse import urljoin, urlparse, parse_qs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from spiders.base_spider import BaseSpider
from utils.browser import get_driver
from utils.progress import CrawlerProgress, print_phase_start, print_phase_complete
from utils.selenium_utils import BrowserPool, safe_click
from config import MAX_WORKERS, PAGE_LOAD_WAIT


class QUBSpider(BaseSpider):
    """
    è´å°”æ³•æ–¯ç‰¹å¥³ç‹å¤§å­¦çˆ¬è™«
    
    è´Ÿè´£ä» Queen's University Belfast å®˜ç½‘çˆ¬å–æ‰€æœ‰ Postgraduate Taught é¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯,åŒ…æ‹¬:
    - é¡¹ç›®åç§°
    - é¡¹ç›®é“¾æ¥
    - ç”³è¯·æˆªæ­¢æ—¥æœŸ(å¦‚æœæœ‰)
    - ç»Ÿä¸€çš„ç”³è¯·æ³¨å†Œå’Œç™»å½•é“¾æ¥
    
    ä½¿ç”¨ç¤ºä¾‹:
        >>> with QUBSpider() as spider:
        ...     data = spider.run()
        ...     print(f"çˆ¬å–äº† {len(data)} æ¡æ•°æ®")
    """
    
    def __init__(self, headless: bool = True, max_workers: int = None):
        """
        åˆå§‹åŒ– QUB çˆ¬è™«
        
        å‚æ•°:
            headless (bool): æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            max_workers (int): å¹¶å‘çº¿ç¨‹æ•°,å¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨ config.py ä¸­çš„é…ç½®
        """
        super().__init__("qub", headless)
        from config import MAX_WORKERS as CONFIG_MAX_WORKERS
        self.max_workers = max_workers if max_workers is not None else CONFIG_MAX_WORKERS
        self.temp_links: List[Dict] = []  # ä¸´æ—¶å­˜å‚¨é¡¹ç›®é“¾æ¥åˆ—è¡¨
        self.progress_manager: CrawlerProgress = None  # è¿›åº¦ç®¡ç†å™¨
        self.browser_pool: BrowserPool = None  # æµè§ˆå™¨æ± 
    
    def run(self) -> List[Dict]:
        """
        æ‰§è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹
        
        æµç¨‹:
            1. Phase 1: è·å–æ‰€æœ‰é¡¹ç›®çš„åˆ—è¡¨(åç§°+é“¾æ¥) - éå†åˆ†é¡µ
            2. Phase 2: å¹¶å‘æŠ“å–æ¯ä¸ªé¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯
        
        è¿”å›:
            List[Dict]: æ‰€æœ‰é¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯åˆ—è¡¨
        """
        self.start_time = time.time()
        self.results = []
        
        try:
            # Phase 1: è·å–é¡¹ç›®åˆ—è¡¨(éå†æ‰€æœ‰åˆ†é¡µ)
            self._fetch_program_list()
            
            if not self.temp_links:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•é¡¹ç›®é“¾æ¥")
                return []
            
            # åˆå§‹åŒ–æµè§ˆå™¨æ± (Phase 2 ä½¿ç”¨)
            self.browser_pool = BrowserPool(size=self.max_workers, headless=True)
            self.browser_pool.initialize()
            
            # Phase 2: å¹¶å‘æŠ“å–è¯¦æƒ…
            self._fetch_program_details()
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†çˆ¬å–")
        except Exception as e:
            print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            # å…³é—­æµè§ˆå™¨æ± 
            if self.browser_pool:
                self.browser_pool.close_all()
            self.close()
        
        self.print_summary()
        return self.results
    
    def _fetch_program_list(self) -> None:
        """
        Phase 1: ä»åˆ—è¡¨é¡µè·å–æ‰€æœ‰é¡¹ç›®çš„åç§°å’Œé“¾æ¥
        
        è¯¥æ–¹æ³•ä¼šéå†æ‰€æœ‰åˆ†é¡µ,æ”¶é›†æ‰€æœ‰é¡¹ç›®çš„åŸºæœ¬ä¿¡æ¯
        QUB ä½¿ç”¨ start_rank å‚æ•°è¿›è¡Œåˆ†é¡µ,æ¯é¡µ100æ¡
        """
        print_phase_start(
            "Phase 1", 
            "æ­£åœ¨æ‰«æé¡¹ç›®åˆ—è¡¨(åˆ†é¡µæ¨¡å¼)...",
            total=None
        )
        print(f"   ğŸ“ ç›®æ ‡åœ°å€: {self.list_url}")
        
        try:
            page_num = 1
            start_rank = 1
            
            while True:
                # æ„å»ºåˆ†é¡µURL
                if page_num == 1:
                    url = self.list_url
                else:
                    url = f"{self.list_url}&start_rank={start_rank}"
                
                print(f"   ğŸ“„ æ­£åœ¨è®¿é—®ç¬¬ {page_num} é¡µ...")
                
                # è®¿é—®é¡µé¢
                self.driver.get(url)
                time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
                
                # ç¬¬ä¸€é¡µæ—¶éœ€è¦å¤„ç†cookie banner
                if page_num == 1:
                    self._handle_cookie_banner()
                
                # ç­‰å¾…è¯¾ç¨‹åˆ—è¡¨åŠ è½½ - ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨
                try:
                    WebDriverWait(self.driver, PAGE_LOAD_WAIT).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, 'ul.search-results'))
                    )
                except TimeoutException:
                    # å°è¯•å¤‡ç”¨é€‰æ‹©å™¨
                    try:
                        WebDriverWait(self.driver, 5).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href*="search.qub.ac.uk/s/redirect"]'))
                        )
                    except TimeoutException:
                        print(f"   âš ï¸ ç¬¬ {page_num} é¡µåŠ è½½è¶…æ—¶,å¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                        break
                
                # æå–å½“å‰é¡µé¢çš„æ‰€æœ‰é¡¹ç›®
                before_count = len(self.temp_links)
                self._extract_programs_from_current_page()
                after_count = len(self.temp_links)
                new_count = after_count - before_count
                
                print(f"   ğŸ“„ ç¬¬ {page_num} é¡µ: å‘ç° {new_count} ä¸ªé¡¹ç›® (ç´¯è®¡: {after_count})")
                
                # å¦‚æœæ²¡æœ‰æ–°é¡¹ç›®,è¯´æ˜å·²åˆ°è¾¾æœ€åä¸€é¡µ
                if new_count == 0:
                    print(f"   âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
                if not self._has_next_page():
                    print(f"   âœ… å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                    break
                
                # å‡†å¤‡ä¸‹ä¸€é¡µ
                page_num += 1
                start_rank += 100
                
                # çŸ­æš‚ä¼‘æ¯,é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)
            
            print_phase_complete("Phase 1", len(self.temp_links))
            
        except Exception as e:
            print(f"âŒ è·å–é¡¹ç›®åˆ—è¡¨å¤±è´¥: {e}")
    
    def _handle_cookie_banner(self) -> None:
        """å¤„ç†Cookieæ¨ªå¹…"""
        try:
            # å°è¯•å¤šç§å¯èƒ½çš„Cookieæ¥å—æŒ‰é’®
            selectors = [
                "//button[contains(text(), 'Accept')]",
                "//button[contains(text(), 'accept')]",
                "//button[contains(@class, 'accept')]",
                "//a[contains(text(), 'Accept')]",
                "//button[@id='onetrust-accept-btn-handler']"
            ]
            
            for selector in selectors:
                try:
                    cookie_btn = WebDriverWait(self.driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, selector))
                    )
                    cookie_btn.click()
                    print("   ğŸª å·²æ¥å—Cookie")
                    time.sleep(1)
                    return
                except TimeoutException:
                    continue
                    
        except Exception as e:
            # Cookieæ¨ªå¹…å¯èƒ½ä¸å­˜åœ¨æˆ–å·²è¢«æ¥å—
            pass
    
    def _has_next_page(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ"""
        try:
            # æŸ¥æ‰¾åˆ†é¡µå¯¼èˆªä¸­çš„ä¸‹ä¸€é¡µæŒ‰é’®æˆ–é“¾æ¥
            next_selectors = [
                "//a[contains(@class, 'next')]",
                "//li[contains(@class, 'next')]/a",
                "//a[@aria-label='Next']",
                "//a[contains(text(), 'â†’')]",
                "//a[contains(text(), '>')]"
            ]
            
            for selector in next_selectors:
                try:
                    next_btn = self.driver.find_element(By.XPATH, selector)
                    if next_btn.is_displayed() and next_btn.is_enabled():
                        return True
                except NoSuchElementException:
                    continue
            
            # å¤‡ç”¨æ–¹æ¡ˆ: æ£€æŸ¥åˆ†é¡µæ•°å­—
            try:
                pagination = self.driver.find_element(By.CSS_SELECTOR, '.pagination, [class*="paging"]')
                current_page = pagination.find_element(By.CSS_SELECTOR, '.active, [aria-current="page"]')
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰åç»­é¡µç 
                all_pages = pagination.find_elements(By.CSS_SELECTOR, 'a')
                return len(all_pages) > 0
            except NoSuchElementException:
                pass
            
            return False
            
        except Exception:
            return False
    
    def _extract_programs_from_current_page(self) -> None:
        """
        ä»å½“å‰é¡µé¢æå–é¡¹ç›®ä¿¡æ¯
        
        QUB ä½¿ç”¨ Funnelback æœç´¢å¼•æ“,è¯¾ç¨‹é“¾æ¥ç»“æ„:
        - é€‰æ‹©å™¨: ul.search-results h4 a
        - çœŸå®URL: åœ¨é“¾æ¥çš„ title å±æ€§ä¸­
        - href: æ˜¯ search.qub.ac.uk/s/redirect é‡å®šå‘é“¾æ¥
        """
        # å»é‡å¤„ç†
        seen_urls = set()
        
        # æ–¹æ³•1: ä½¿ç”¨æ­£ç¡®çš„é€‰æ‹©å™¨ - ul.search-results h4 a
        course_links = self.driver.find_elements(
            By.CSS_SELECTOR, 
            'ul.search-results h4 a'
        )
        
        # å¦‚æœæ²¡æ‰¾åˆ°,å°è¯•å¤‡ç”¨é€‰æ‹©å™¨
        if not course_links:
            course_links = self.driver.find_elements(
                By.CSS_SELECTOR, 
                'h4 a[href*="search.qub.ac.uk/s/redirect"]'
            )
        
        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°,å°è¯•æ›´å®½æ³›çš„é€‰æ‹©å™¨
        if not course_links:
            course_links = self.driver.find_elements(
                By.CSS_SELECTOR,
                'a[href*="search.qub.ac.uk/s/redirect"]'
            )
        
        for link in course_links:
            try:
                # è·å–è¯¾ç¨‹åç§°
                name = link.text.strip()
                
                if not name or len(name) < 3:
                    continue
                    
                # è¿‡æ»¤æ— æ•ˆçš„é“¾æ¥æ–‡æœ¬
                if name.lower() in ['next', 'previous', '>', '<', '1', '2', '3', 'â†’']:
                    continue
                
                # ä¼˜å…ˆä» title å±æ€§è·å–çœŸå®URL
                real_url = link.get_attribute("title")
                
                # å¦‚æœ title ä¸ºç©º,ä» href çš„ url å‚æ•°ä¸­æå–
                if not real_url or not real_url.startswith('http'):
                    href = link.get_attribute("href")
                    if href:
                        real_url = self._extract_real_url(href)
                
                if not real_url:
                    continue
                
                # ç¡®ä¿URLæ ¼å¼æ­£ç¡®
                if not real_url.startswith('http'):
                    continue
                    
                # è·³è¿‡å·²å¤„ç†çš„URL
                if real_url in seen_urls:
                    continue
                seen_urls.add(real_url)
                
                # å»é‡æ£€æŸ¥ (ä¸å·²æœ‰åˆ—è¡¨æ¯”å¯¹)
                if not any(d['link'] == real_url for d in self.temp_links):
                    self.temp_links.append({
                        "name": name,
                        "link": real_url
                    })
                    
            except Exception:
                continue
    
    def _extract_real_url(self, url: str) -> str:
        """
        ä»é‡å®šå‘URLä¸­æå–çœŸå®çš„è¯¾ç¨‹URL
        
        QUBçš„æœç´¢ç»“æœé“¾æ¥æ ¼å¼:
        https://search.qub.ac.uk/s/redirect?...&url=https%3A%2F%2Fwww.qub.ac.uk%2Fcourses%2F...
        """
        try:
            if 'search.qub.ac.uk/s/redirect' in url:
                # è§£ææŸ¥è¯¢å‚æ•°è·å–çœŸå®URL
                from urllib.parse import urlparse, parse_qs, unquote
                parsed = urlparse(url)
                params = parse_qs(parsed.query)
                if 'url' in params:
                    return unquote(params['url'][0])
            return url
        except Exception:
            return url
    
    def _fetch_program_details(self) -> None:
        """
        Phase 2: å¹¶å‘æŠ“å–æ‰€æœ‰é¡¹ç›®çš„è¯¦ç»†ä¿¡æ¯
        
        ä½¿ç”¨è¿›åº¦ç®¡ç†å™¨å’Œæµè§ˆå™¨æ± æ‰§è¡Œå¹¶å‘ä»»åŠ¡
        """
        # åˆ›å»ºè¿›åº¦ç®¡ç†å™¨
        self.progress_manager = CrawlerProgress(max_workers=self.max_workers)
        
        # æ‰§è¡Œå¹¶å‘æŠ“å–
        self.results = self.progress_manager.run_tasks(
            items=self.temp_links,
            task_func=self._process_single_program,
            task_name="æŠ“å–è¿›åº¦",
            phase_name="Phase 2"
        )
    
    def _process_single_program(self, item: Dict) -> tuple:
        """
        å¤„ç†å•ä¸ªé¡¹ç›®çš„è¯¦æƒ…é¡µæŠ“å–
        
        ä½¿ç”¨æµè§ˆå™¨æ± å¤ç”¨æµè§ˆå™¨å®ä¾‹,æå‡æ€§èƒ½
        
        å‚æ•°:
            item (Dict): åŒ…å« name å’Œ link çš„é¡¹ç›®ä¿¡æ¯
        
        è¿”å›:
            tuple: (ç»“æœå­—å…¸, è€—æ—¶ç§’æ•°)
        """
        item_start = time.time()
        
        # åˆ›å»ºç»“æœæ¨¡æ¿
        result = self.create_result_template(item['name'], item['link'])
        
        # è®¾ç½®ç»Ÿä¸€çš„ç”³è¯·é“¾æ¥(ä»é…ç½®è¯»å–)
        result["ç”³è¯·é“¾æ¥"] = self.university_info.get("apply_register_url", "N/A")
        
        # ä»æµè§ˆå™¨æ± è·å–å®ä¾‹
        with self.browser_pool.get_browser() as driver:
            try:
                # è®¿é—®é¡¹ç›®è¯¦æƒ…é¡µ
                driver.get(item['link'])
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "main"))
                )
                
                # æŠ“å–deadlineä¿¡æ¯
                result["é¡¹ç›®deadline"] = self._extract_deadline(driver)
                
                # å°è¯•æŠ“å–å¼€æ”¾æ—¥æœŸ
                result["é¡¹ç›®opendate"] = self._extract_open_date(driver)
                
            except TimeoutException:
                # è¯¦æƒ…é¡µåŠ è½½è¶…æ—¶,ä¿æŒN/A
                pass
            except Exception:
                # å…¶ä»–é”™è¯¯,ä¿æŒN/A
                pass
        
        duration = time.time() - item_start
        return result, duration
    
    def _extract_deadline(self, driver) -> str:
        """
        æå–ç”³è¯·æˆªæ­¢æ—¥æœŸ
        """
        try:
            # æŸ¥æ‰¾åŒ…å«deadlineå…³é”®è¯çš„å…ƒç´ 
            keywords = ['deadline', 'closing date', 'application close', 'apply by']
            
            for keyword in keywords:
                try:
                    elements = driver.find_elements(
                        By.XPATH, 
                        f"//*[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{keyword}')]"
                    )
                    
                    for elem in elements:
                        text = elem.text.strip()
                        if text and len(text) > 5 and len(text) < 500:
                            # å°è¯•æ‰¾åˆ°æ—¥æœŸä¿¡æ¯
                            parent = elem.find_element(By.XPATH, "./..")
                            parent_text = parent.text.strip()
                            if parent_text and len(parent_text) < 500:
                                return parent_text
                            return text
                            
                except NoSuchElementException:
                    continue
            
            # å¤‡ç”¨æ–¹æ¡ˆ: æŸ¥æ‰¾ "How to apply" æˆ– "Apply" éƒ¨åˆ†
            try:
                apply_section = driver.find_element(
                    By.XPATH, 
                    "//h2[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'apply')]/.."
                )
                section_text = apply_section.text
                
                lines = section_text.split('\n')
                for line in lines:
                    if 'deadline' in line.lower() or 'date' in line.lower():
                        return line.strip()
                        
            except NoSuchElementException:
                pass
            
            return "N/A"
            
        except Exception:
            return "N/A"
    
    def _extract_open_date(self, driver) -> str:
        """
        æå–ç”³è¯·å¼€æ”¾æ—¥æœŸ
        """
        try:
            keywords = ['open date', 'opening date', 'applications open', 'apply from']
            
            for keyword in keywords:
                try:
                    elements = driver.find_elements(
                        By.XPATH, 
                        f"//*[contains(translate(text(), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{keyword}')]"
                    )
                    
                    for elem in elements:
                        text = elem.text.strip()
                        if text and len(text) > 5 and len(text) < 300:
                            return text
                            
                except NoSuchElementException:
                    continue
            
            return "N/A"
            
        except Exception:
            return "N/A"


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    with QUBSpider(headless=False) as spider:
        results = spider.run()
        
        print(f"\næŠ“å–å®Œæˆ,å…± {len(results)} ä¸ªé¡¹ç›®")
        if results:
            import json
            print("\nå‰3ä¸ªé¡¹ç›®ç¤ºä¾‹:")
            print(json.dumps(results[:3], indent=2, ensure_ascii=False))
