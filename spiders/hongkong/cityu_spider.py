# -*- coding: utf-8 -*-
"""
é¦™æ¸¯åŸŽå¸‚å¤§å­¦ (CityU) çˆ¬è™«
ç›®æ ‡ç½‘å€: https://www.cityu.edu.hk/pg/taught-postgraduate-programmes/list
"""

import time
from typing import List, Dict, Tuple
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from spiders.base_spider import BaseSpider
from utils.progress import CrawlerProgress

class CityUSpider(BaseSpider):
    def __init__(self, headless: bool = True):
        # CityU æœ‰æžå…¶ä¸¥æ ¼çš„ Incapsula WAF é˜²æŠ¤ï¼Œæ— å¤´æ¨¡å¼(Headless)å‡ ä¹Žå¿…æ­»
        # å› æ­¤è¿™é‡Œå¼ºåˆ¶è¦†ç›–ä¼ å…¥çš„ headless å‚æ•°ï¼Œå¿…é¡»ä½¿ç”¨æœ‰å¤´æ¨¡å¼
        print("ðŸ›¡ï¸ æ£€æµ‹åˆ° CityU å®‰å…¨é˜²æŠ¤ï¼Œå¼ºåˆ¶åˆ‡æ¢ä¸ºæœ‰å¤´æ¨¡å¼(Headful)ä»¥ç»•è¿‡æ‹¦æˆª...")
        super().__init__("cityu", headless=False)
        # å›ºå®šç”³è¯·é“¾æŽ¥
        self.apply_url = "https://banweb.cityu.edu.hk/pls/PROD/hwskalog_cityu.P_DispLoginNon"

    def run(self) -> List[Dict]:
        """
        æ‰§è¡Œçˆ¬è™«ä¸»æµç¨‹
        """
        print(f"ðŸš€ å¼€å§‹æŠ“å– {self.university_info['name']}...")
        print(f"ðŸ“ åˆ—è¡¨é¡µ: {self.university_info['list_url']}")
        
        self.results = []
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†é¡¹ç›®é“¾æŽ¥
            print("ðŸ” æ­£åœ¨æ”¶é›†é¡¹ç›®åˆ—è¡¨...")
            items = self._collect_program_links()
            
            if not items:
                print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•é¡¹ç›®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é€‰æ‹©å™¨")
                return []
            
            print(f"ðŸ“¦ å…±å‘çŽ° {len(items)} ä¸ªé¡¹ç›®ï¼Œå‡†å¤‡æŠ“å–è¯¦æƒ…...")
            
            # ç¬¬äºŒé˜¶æ®µï¼šå¹¶å‘æŠ“å–è¯¦æƒ…
            progress = CrawlerProgress(
                max_workers=self.university_info.get("max_workers", 12)
            )
            
            self.results = progress.run_tasks(
                items=items,
                task_func=self._fetch_details,
                task_name="æŠ“å–è¯¦æƒ…",
                phase_name="CityU Details"
            )
            
        finally:
            # BaseSpider ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¼šè‡ªåŠ¨å…³é—­ä¸» driverï¼Œæ— éœ€æ‰‹åŠ¨å…³é—­
            pass
            
        # æ•°æ®åŽ»é‡
        from utils.deduplicator import deduplicate_results
        self.results = deduplicate_results(self.results)
        
        self.print_summary()
        return self.results

    def _collect_program_links(self) -> List[Dict]:
        """
        ä»Žåˆ—è¡¨é¡µæ”¶é›†æ‰€æœ‰é¡¹ç›®é“¾æŽ¥
        ç­–ç•¥: å¿«é€Ÿæ‹ç…§æ³• - ä¸€æ¬¡æ€§æŠ“å– HTMLï¼Œé¿å…é¢‘ç¹ Selenium è°ƒç”¨è§¦å‘ WAF
        """
        self.driver.get(self.university_info['list_url'])
        
        # å¢žåŠ é€šç”¨ç­‰å¾…
        try:
            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        except:
            print("âš ï¸ é¡µé¢åŠ è½½åŸºç¡€å†…å®¹è¶…æ—¶")
        
        # ç­‰å¾…è¡¨æ ¼åŠ è½½
        try:
            WebDriverWait(self.driver, 20).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.table-responsive"))
            )
            # ç¨ä½œç­‰å¾…ç¡®ä¿æ¸²æŸ“å®Œæˆ
            time.sleep(5)
        except Exception as e:
            print(f"âš ï¸ ç­‰å¾…é¡µé¢è¡¨æ ¼åŠ è½½è¶…æ—¶: {e}")
            return []

        # ðŸŽ¯ å…³é”®ä¼˜åŒ–ï¼šä¸€æ¬¡æ€§æŠ“å–æ•´ä¸ªé¡µé¢æºä»£ç 
        print("ðŸ“¸ æ­£åœ¨æŠ“å–é¡µé¢å¿«ç…§ï¼ˆé¿å…è§¦å‘é˜²ç«å¢™ï¼‰...")
        page_html = self.driver.page_source
        
        # ä½¿ç”¨ BeautifulSoup ç¦»çº¿è§£æž
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_html, 'html.parser')
        
        items = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å­¦é™¢çš„è¡¨æ ¼å®¹å™¨
        college_containers = soup.select("div.table-responsive")
        print(f"DEBUG: æ‰¾åˆ° {len(college_containers)} ä¸ªè¡¨æ ¼å®¹å™¨")
        
        for i, container in enumerate(college_containers):
            try:
                # èŽ·å–å­¦é™¢åç§°
                college_code = container.get("data-college") or f"æœªçŸ¥å­¦é™¢_{i}"
                
                # æŸ¥æ‰¾è¡¨æ ¼è¡Œ
                rows = container.select("tr")
                
                for row in rows:
                    try:
                        # æŸ¥æ‰¾é¡¹ç›®é“¾æŽ¥å’Œåç§°
                        link_els = row.select("td.col-prog-title a")
                        
                        if not link_els:
                            continue
                            
                        link_el = link_els[0]
                        name = self._clean_text(link_el.get_text(strip=True))
                        url = link_el.get("href")
                        
                        if not name or not url:
                            continue
                        
                        # å¤„ç†ç›¸å¯¹é“¾æŽ¥
                        if url.startswith("/"):
                            url = self.university_info['base_url'] + url
                            
                        items.append({
                            "name": name,
                            "link": url,
                            "college": college_code
                        })
                    except Exception as e:
                        # ä¸å†æ‰“å°æ¯ä¸€è¡Œçš„é”™è¯¯ï¼ˆå¤ªåµï¼‰
                        continue
                        
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å­¦é™¢è¡¨æ ¼æ—¶å‡ºé”™: {e}")
                continue
                
        print(f"âœ… æ€»å…±æ”¶é›†åˆ° {len(items)} ä¸ªé¡¹ç›®")
        return items

    def _fetch_details(self, item: Dict) -> Tuple[Dict, float]:
        """
        æŠ“å–å•ä¸ªé¡¹ç›®è¯¦æƒ…
        """
        # æ³¨æ„ï¼šè¿™é‡Œä¼šç”±å¤šçº¿ç¨‹è°ƒç”¨ï¼Œæ¯ä¸ªçº¿ç¨‹éœ€è¦åˆ›å»ºè‡ªå·±çš„ driver 
        # ä½† BaseSpider ç›®å‰è®¾è®¡æ˜¯å• driver æ¨¡å¼
        # è¿™é‡Œçš„ fetch_details è®¾è®¡éœ€è¦éµå¾ª CrawlerProgress çš„æ¨¡å¼
        # æˆ‘ä»¬ä½¿ç”¨ä¸´æ—¶ driver
        
        from utils.browser import get_driver, close_driver
        
        start_time = time.time()
        result = self.create_result_template(item["name"], item["link"])
        result["é¡¹ç›®ç”³è¯·é“¾æŽ¥"] = self.apply_url
        
        # å¯åŠ¨ä¸´æ—¶æµè§ˆå™¨
        driver = get_driver(headless=True)
        
        try:
            driver.get(item['link'])
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "h1"))
                )
            except:
                pass
            
            # è¿™é‡Œéœ€è¦æ ¹æ®å®žé™…é¡µé¢ç»“æž„è§£æž
            # CityU è¯¦æƒ…é¡µé€šå¸¸åŒ…å«ï¼š
            # - Application Deadline (é€šå¸¸åœ¨è¡¨æ ¼ä¸­æˆ–æ–‡æœ¬ä¸­)
            # - Combined mode / Full-time mode details
            
            # èŽ·å–é¡µé¢æ‰€æœ‰æ–‡æœ¬ç”¨äºŽç®€å•åŒ¹é…
            page_text = driver.find_element(By.TAG_NAME, "body").text
            
            # å°è¯•æŸ¥æ‰¾ Deadline
            # å¸¸è§æ ¼å¼: "Application Deadline: 31 May 2024" æˆ–è¡¨æ ¼ä¸­çš„ "Application Deadline"
            deadline = self._extract_deadline(driver, page_text)
            if deadline:
                result["é¡¹ç›®deadline"] = deadline
            
        except Exception as e:
            result["_error"] = str(e)
        finally:
            close_driver(driver)
            
        duration = time.time() - start_time
        return result, duration

    def _extract_deadline(self, driver, page_text: str) -> str:
        """
        å°è¯•æå– Application Deadline
        """
        # ç­–ç•¥ 1: æŸ¥æ‰¾åŒ…å« "Application Deadline" çš„è¡¨æ ¼è¡Œ
        try:
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å« "Deadline" çš„ th æˆ– td
            deadline_labels = driver.find_elements(By.XPATH, "//*[contains(text(), 'Application Deadline')]")
            for label in deadline_labels:
                # å°è¯•æ‰¾åŒè¡Œçš„ä¸‹ä¸€ä¸ª tdï¼Œæˆ–è€…çˆ¶çº§ tr
                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æ ¼çš„ä¸€éƒ¨åˆ†
                    parent_tr = label.find_element(By.XPATH, "./ancestor::tr")
                    # èŽ·å–è¯¥è¡Œæ‰€æœ‰æ–‡æœ¬
                    row_text = parent_tr.text
                    # ç®€å•çš„æ–‡æœ¬å¤„ç†: "Application Deadline 30 April 2024" -> "30 April 2024"
                    clean_text = row_text.replace("Application Deadline", "").replace("Closing Date", "").strip()
                    if len(clean_text) > 5:
                        return clean_text
                except:
                    continue
        except:
            pass
            
        # ç­–ç•¥ 2: åœ¨å…¨æ–‡ä¸­æœç´¢
        # (ç®€å•å®žçŽ°ï¼ŒåŽç»­å¯æ ¹æ®å®žé™… HTML ä¼˜åŒ–)
        lines = page_text.split('\n')
        for line in lines:
            if "Application Deadline" in line or "Closing Date" in line:
                # ç®€å•çš„æˆªå–
                parts = line.split(":")
                if len(parts) > 1:
                    val = parts[1].strip()
                    if len(val) > 5 and len(val) < 100:
                        return val
                        
        return "N/A"
