# -*- coding: utf-8 -*-
import time
import requests
import re
import concurrent.futures
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional, Set
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from spiders.base_spider import BaseSpider
from config import UNIVERSITY_INFO
from utils.progress import CrawlerProgress

class NYUSpider(BaseSpider):
    """
    çº½çº¦å¤§å­¦ (NYU) çˆ¬è™« (US018)
    
    ç»“æ„:
    1. è®¿é—® Bulletins é¡µé¢ï¼Œæå–é¡¹ç›®åˆ—è¡¨
    2. åŠ¨æ€æ„å»º School æ˜ å°„
    3. å¹¶å‘æ·±åº¦çˆ¬å– (Deep Scraping) æ¯ä¸ªé¡¹ç›®é¡µé¢ä»¥è·å– Application Link
       - åŒ…å«ç‰¹å®šé“¾æ¥æ›¿æ¢è§„åˆ™
       - åŒ…å«é€’å½’æŸ¥æ‰¾ "how-to-apply" é¡µé¢
       - éªŒè¯ç”³è¯·é“¾æ¥åŒ…å« "log in" æˆ– "create an account"
    """

    # æ›¿æ¢è§„åˆ™è¡¨ (User specified)
    REPLACEMENT_RULES = {
        "apply.steinhardt.nyu.edu/portal/graduate_application": "https://apply.steinhardt.nyu.edu/portal/graduate_application?_ga=2.4617783.507695171.1768439682-1008657193.1768439677",
        "docs.google.com/forms/d/e/1FAIpQLSfjCR_pZAph-bmp5eTO_gXj2UjrUq5_FqkzTUs-78A4Sak4zQ/viewform": "https://apply.steinhardt.nyu.edu/portal/graduate_application?_ga=2.4617783.507695171.1768439682-1008657193.1768439677",
        "www.law.nyu.edu/graduateadmissions": "https://llm.lsac.org/login/access.aspx",
        "www.sps.nyu.edu/join/apply-now/apply-now-undergraduate-degrees.html": "https://apply.sps.nyu.edu/apply/?sr=9044ec14-eb84-4289-ab43-44e5c2df4f87",
        "www.nysed.gov/heds/irpsl1.html": "ä¸æ¥å—ç”³è¯·",
        "gallatin.nyu.edu/admissions/graduate/applying.html": "https://apply.gallatin.nyu.edu/apply/"
    }

    # å­¦é™¢çº§ Fallback é“¾æ¥ (å½“æ‰¾ä¸åˆ°å…·ä½“é¡¹ç›®ç”³è¯·é“¾æ¥æ—¶ä¼˜å…ˆä½¿ç”¨)
    SCHOOL_FALLBACK_LINKS = {
        "Stern": "https://admissions.stern.nyu.edu/apply/",
        "Steinhardt": "https://apply.steinhardt.nyu.edu/portal/graduate_application",
        "Tisch": "https://apply.tisch.nyu.edu/apply/",
        "GSAS": "https://apply.gsas.nyu.edu/apply/",
        "Tandon": "https://apply.tandon.nyu.edu/apply/",
        "Wagner": "https://apply.wagner.nyu.edu/apply/",
        "Silver": "https://apply.socialwork.nyu.edu/apply/",
        "Gallatin": "https://apply.gallatin.nyu.edu/apply/",
        "SPS": "https://apply.sps.nyu.edu/apply/"
    }

    # Deep Scraping Config
    TIMEOUT = 15
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    def __init__(self, headless: bool = True):
        super().__init__("nyu", headless=headless)
        self.config = UNIVERSITY_INFO["nyu"]
        self.programs = []
        self.start_url = "https://bulletins.nyu.edu/programs/#filter=.filter_55"
        self.school_mapping = {}

    def _build_school_mapping(self, soup: BeautifulSoup):
        """
        ä»å·¦ä¾§ç­›é€‰å™¨æ„å»º filter class åˆ° School Name çš„æ˜ å°„
        """
        print("ğŸ” æ­£åœ¨æ„å»ºå­¦é™¢æ˜ å°„è¡¨...")
        try:
            school_fieldset = None
            fieldsets = soup.find_all("fieldset")
            for fs in fieldsets:
                legend = fs.find("legend")
                if legend and "School" in legend.get_text():
                    school_fieldset = fs
                    break
            
            if not school_fieldset:
                print("âš ï¸ æœªæ‰¾åˆ° School ç­›é€‰åŒºåŸŸ")
                return

            filters = school_fieldset.select("div.filters__filter")
            for f in filters:
                input_tag = f.find("input")
                label_tag = f.find("label")
                
                if input_tag and label_tag:
                    value = input_tag.get("value", "")
                    name = label_tag.get_text(strip=True)
                    if value.startswith(".filter_"):
                        key = value.replace(".", "")
                        self.school_mapping[key] = name
            
            print(f"âœ… æ„å»ºæ˜ å°„è¡¨å®Œæˆï¼Œå…± {len(self.school_mapping)} é¡¹")
        except Exception as e:
            print(f"âš ï¸ æ„å»ºå­¦é™¢æ˜ å°„è¡¨å¤±è´¥: {e}")

    def _deep_scrape_program(self, program_item: Dict) -> Dict:
        """
        ä½¿ç”¨ DeepCrawler è¿›è¡Œæ·±åº¦æŠ“å–
        """
        from utils.deep_crawler import DeepCrawler
        
        start_url = program_item["é¡¹ç›®å®˜ç½‘é“¾æ¥"]
        crawler = DeepCrawler(max_depth=3)
        result = crawler.crawl(start_url)
        
        program_item["é¡¹ç›®deadline"] = result["deadline"]
        apply_link = result["apply_link"]
        
        # 1. Check User Replacement Rules
        for key, val in self.REPLACEMENT_RULES.items():
            if key in apply_link:
                apply_link = val
                break
                
        # 2. Check Validity & School Fallback
        if apply_link == "N/A" or "how-to-apply" in apply_link:
            # Try school fallback
            school = program_item.get("å­¦é™¢/å­¦ä¹ é¢†åŸŸ", "")
            for key, val in self.SCHOOL_FALLBACK_LINKS.items():
                if key in school:
                    apply_link = val
                    break
        
        # 3. Final Fallback to Config Default (if still needed, mostly handled by school fallback)
        if apply_link == "N/A":
             apply_link = self.config.get("apply_register_url", "N/A")

        program_item["ç”³è¯·é“¾æ¥"] = apply_link
             
        # Fallback logic for Deadline
        if program_item["é¡¹ç›®deadline"] == "N/A":
             program_item["é¡¹ç›®deadline"] = "See Program Website"
             
        return program_item

    def run(self) -> List[Dict]:
        print(f"ğŸ“„ å¼€å§‹çˆ¬å– {self.school_name} çš„ä¸“ä¸šä¿¡æ¯...")
        driver = self.driver
        
        try:
            # Phase 1: Access the main page
            print(f"ğŸ“„ æ­£åœ¨è®¿é—®: {self.start_url}")
            driver.get(self.start_url)
            
            # ç­‰å¾…åˆ—è¡¨åŠ è½½
            print("â³ ç­‰å¾…é¡¹ç›®åˆ—è¡¨åŠ è½½...")
            try:
                WebDriverWait(driver, 25).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#filters-grid-tab-content a"))
                )
                time.sleep(5)
            except Exception as e:
                print(f"âš ï¸ ç­‰å¾…åˆ—è¡¨åŠ è½½è¶…æ—¶æˆ–å‡ºé”™: {e}")
            
            # Phase 2: Extract data basics
            print("ğŸ“„ å¼€å§‹æå–é¡¹ç›®åˆ—è¡¨...")
            content = driver.page_source
            soup = BeautifulSoup(content, 'html.parser')
            
            self._build_school_mapping(soup)
            
            program_links = soup.select("#filters-grid-tab-content a")
            print(f"ğŸ” é¡µé¢å…±æ‰¾åˆ° {len(program_links)} ä¸ªæ½œåœ¨é¡¹ç›®é“¾æ¥")

            initial_items = []
            
            for link in program_links:
                try:
                    li_element = link.find_parent("li")
                    if not li_element:
                        continue
                    classes = li_element.get("class", [])
                    if "filter_55" not in classes:
                        continue
                        
                    title_span = link.select_one("span.title")
                    if not title_span:
                        continue
                    program_name = title_span.get_text(strip=True)
                    
                    href = link.get("href", "")
                    full_link = f"https://bulletins.nyu.edu{href}" if href.startswith("/") else href
                        
                    school_name = "New York University"
                    ignored_filters = {"filter_55", "filter_1", "filter_2", "filter_3", "filter_4"}

                    for cls in classes:
                        if cls in self.school_mapping and cls not in ignored_filters:
                            candidate_name = self.school_mapping[cls]
                            if candidate_name not in ["Graduate", "Undergraduate", "In Person", "Online", "Masters", "Doctoral"]:
                                school_name = candidate_name
                                break
                    
                    program_item = {
                        "å­¦æ ¡ä»£ç ": self.config["code"],
                        "å­¦æ ¡åç§°": self.config["name"],
                        "é¡¹ç›®åç§°": program_name,
                        "å­¦é™¢/å­¦ä¹ é¢†åŸŸ": school_name,
                        "é¡¹ç›®å®˜ç½‘é“¾æ¥": full_link,
                        "ç”³è¯·é“¾æ¥": "Searching...", # Placeholder
                        "é¡¹ç›®opendate": "",
                        "é¡¹ç›®deadline": "See Program Website",
                        "å­¦ç”Ÿæ¡ˆä¾‹": "",
                        "é¢è¯•é—®é¢˜": ""
                    }
                    initial_items.append(program_item)
                    
                except Exception as e:
                    print(f"âŒ è§£æå•é¡¹åŸºç¡€ä¿¡æ¯å‡ºé”™: {e}")

            print(f"âœ… åŸºç¡€ä¿¡æ¯æå–å®Œæˆï¼Œå…± {len(initial_items)} ä¸ªé¡¹ç›®ã€‚å¼€å§‹å¹¶å‘æ·±åº¦çˆ¬å–...")
            
            # Phase 3: Concurrent Deep Scraping
            def deep_scrape_wrapper(item):
                start = time.time()
                result = self._deep_scrape_program(item)
                return result, time.time() - start

            progress = CrawlerProgress(max_workers=24) # Increased workers for speed
            self.programs = progress.run_tasks(
                items=initial_items,
                task_func=deep_scrape_wrapper,
                task_name="Deep Scraping",
                phase_name="æ·±åº¦æŠ“å–"
            )
            
            print(f"âœ… æ‰€æœ‰é¡¹ç›®çˆ¬å–å®Œæˆï¼Œå…± {len(self.programs)} ä¸ª")

        except Exception as e:
            print(f"âŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")
        
        return self.programs

if __name__ == "__main__":
    spider = NYUSpider(headless=True)
    spider.run()
