# -*- coding: utf-8 -*-
"""
å“ˆä½›å¤§å­¦ (Harvard University) çˆ¬è™«æ¨¡å—
è´Ÿè´£æŠ“å– Harvard Graduate é¡¹ç›®ä¿¡æ¯

æå–è§„åˆ™ï¼š
  - Phase 1: ä»åˆ—è¡¨é¡µ (Page 1-9) è·å–æ‰€æœ‰å¤§ç±»åŠå…¶è¯¦æƒ…é¡µé“¾æ¥
  - Phase 2: å¹¶å‘ (24çº¿ç¨‹) å¤„ç†æ¯ä¸ªå¤§ç±»ï¼š
      1. ç›´æ¥è®¿é—®å¤§ç±»è¯¦æƒ…é¡µ (æ— éœ€åœ¨åˆ—è¡¨é¡µç‚¹å‡»)
      2. æ‰¾åˆ°å¹¶å±•å¼€ "Graduate" æŠ˜å é¡µ
      3. æå– "Graduate" ä¸‹çš„æ‰€æœ‰å­é¡¹ç›® (Name, School, LearnMoreURL)
      4. ä¾æ¬¡è®¿é—® LearnMoreURLï¼Œæå– Deadline
      5. ç»„åˆæœ€ç»ˆæ•°æ®
"""

import sys
import time
import re
import concurrent.futures
from typing import List, Dict, Any, Tuple
from urllib.parse import urljoin

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from selenium.webdriver.remote.webdriver import WebDriver

from spiders.base_spider import BaseSpider
from utils.progress import print_phase_start, print_phase_complete
from utils.selenium_utils import BrowserPool, safe_click
from config import PAGE_LOAD_WAIT, MAX_WORKERS

# å…¨å±€é…ç½®ï¼Œå…è®¸å¤–éƒ¨è¦†ç›–
HARVARD_MAX_WORKERS = 24

def log(msg: str):
    """å¸¦åˆ·æ–°çš„æ‰“å°å‡½æ•°ï¼Œç¡®ä¿å³æ—¶æ˜¾ç¤º"""
    print(msg, flush=True)


class HarvardSpider(BaseSpider):
    """
    å“ˆä½›å¤§å­¦çˆ¬è™«
    
    æå–æ‰€æœ‰ç ”ç©¶ç”Ÿé¡¹ç›®çš„å®é™…å­é¡¹ç›®ä¿¡æ¯
    """
    
    def __init__(self, headless: bool = True, max_workers: int = None):
        super().__init__("harvard", headless)
        self.max_workers = max_workers or HARVARD_MAX_WORKERS
        self.categories = []  # å­˜å‚¨å¤§ç±»ä¿¡æ¯
        self.programs_collected = []  # å­˜å‚¨æœ€ç»ˆé¡¹ç›®
        self.browser_pool = None
    
    def run(self) -> List[Dict]:
        """
        æ‰§è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹
        """
        self.start_time = time.time()
        
        log(f"\n{'='*60}")
        log(f"ğŸ“ å¼€å§‹çˆ¬å–: {self.university_info['name_cn']}")
        log(f"ğŸ“ ç›®æ ‡åœ°å€: {self.list_url}")
        log(f"{'='*60}\n")
        
        try:
            # Phase 1: æ”¶é›†æ‰€æœ‰å¤§ç±»ç´¢å¼•
            print_phase_start("Phase 1", "æ”¶é›†æ‰€æœ‰å¤§ç±»ç´¢å¼• (Pages 1-9)")
            self._collect_all_categories()
            print_phase_complete("Phase 1", len(self.categories))
            
            # Phase 2: å¹¶å‘æå–è¯¦æƒ…
            if self.categories:
                print_phase_start("Phase 2", "æå–å­é¡¹ç›®è¯¦æƒ… & Deadline", total=len(self.categories))
                self._extract_all_subprograms_concurrent()
                print_phase_complete("Phase 2", len(self.programs_collected))
            
            # è®¾ç½®ç»“æœ
            self.results = self.programs_collected
            
        except Exception as e:
            log(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if self.browser_pool:
                self.browser_pool.close_all()
        
        self.print_summary()
        return self.results
    
    def _collect_all_categories(self):
        """Phase 1: ä»æ‰€æœ‰åˆ†é¡µæ”¶é›†å¤§ç±»ç´¢å¼•ä¿¡æ¯"""
        total_pages = 9 
        
        for page_num in range(1, total_pages + 1):
            log(f"   ğŸ“„ æ­£åœ¨æ”¶é›†ç¬¬ {page_num}/{total_pages} é¡µçš„å¤§ç±»...")
            self._collect_categories_from_page_url(page_num)
        
        log(f"   âœ… å…±æ”¶é›† {len(self.categories)} ä¸ªå¤§ç±» (é¢„æœŸ ~134 ä¸ª)")
    
    def _collect_categories_from_page_url(self, page_num: int):
        """ç›´æ¥è®¿é—®æŒ‡å®šé¡µç çš„ URL æ”¶é›†å¤§ç±»"""
        for attempt in range(3):
            try:
                # æ„é€ å¸¦é¡µç çš„ URL
                target_url = f"{self.list_url}&page={page_num}"
                self.driver.set_page_load_timeout(15) # è®¾ç½®è¾ƒçŸ­è¶…æ—¶ï¼Œé˜²æ­¢å¡æ­»
                try:
                    self.driver.get(target_url)
                except TimeoutException:
                    pass
                
                # æ¢å¤é»˜è®¤è¶…æ—¶
                self.driver.set_page_load_timeout(60)
                
                try:
                    WebDriverWait(self.driver, 20).until(
                        EC.presence_of_all_elements_located((By.CSS_SELECTOR, "div.c-programs-item"))
                    )
                except TimeoutException:
                    if attempt < 2:
                        continue
                    log(f"   âš ï¸ ç¬¬ {page_num} é¡µåŠ è½½è¶…æ—¶æˆ–æ— å†…å®¹")
                    return

                # ä¸´æ—¶é™ä½éšå¼ç­‰å¾…ä»¥åŠ é€ŸæŸ¥æ‰¾ä¸å­˜åœ¨çš„å…ƒç´ 
                self.driver.implicitly_wait(0.1)
                
                items = self.driver.find_elements(By.CSS_SELECTOR, "div.c-programs-item")
                
                count_on_page = 0
                for idx, item in enumerate(items):
                    try:
                        # å§“å
                        try:
                            name_elem = item.find_element(By.CSS_SELECTOR, "h2.c-programs-item__title")
                            name = name_elem.text.strip()
                        except NoSuchElementException:
                            # å°è¯•æ‰¾ä»»ä½• h2
                            try:
                                name = item.find_element(By.TAG_NAME, "h2").text.strip()
                            except:
                                continue

                        # é“¾æ¥
                        url = None
                        try:
                            # ç›´æ¥åœ¨ item ä¸‹æ‰¾ a
                            link_elem = item.find_element(By.TAG_NAME, "a")
                            url = link_elem.get_attribute("href")
                        except NoSuchElementException:
                            pass
                            
                        if not name:
                            continue
                            
                        self.categories.append({
                            "name": name,
                            "url": url,
                            "page_num": page_num
                        })
                        count_on_page += 1
                    except Exception:
                        continue
                
                # æ¢å¤éšå¼ç­‰å¾…
                self.driver.implicitly_wait(5)
                
                # æˆåŠŸæ”¶é›†ï¼Œè·³å‡ºé‡è¯•
                return
                
            except Exception as e:
                log(f"   âš ï¸ æ”¶é›†ç¬¬ {page_num} é¡µ (å°è¯• {attempt+1}/3) å‡ºé”™: {e}")
                time.sleep(2)

    def _extract_all_subprograms_concurrent(self):
        """Phase 2: ä½¿ç”¨ BrowserPool å¹¶å‘æå–è¯¦æƒ…"""
        total = len(self.categories)
        log(f"   ğŸ“Š å¼€å§‹å¤„ç† {total} ä¸ªå¤§ç±» (ä½¿ç”¨ {self.max_workers} çº¿ç¨‹)...")
        
        self.browser_pool = BrowserPool(size=self.max_workers, headless=self.headless)
        self.browser_pool.initialize()
        
        extracted_count = 0
        current_done = 0
        
        # ä½¿ç”¨ ThreadPoolExecutor
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_cat = {
                executor.submit(self._process_single_category, cat): cat 
                for cat in self.categories
            }
            
            for future in concurrent.futures.as_completed(future_to_cat):
                try:
                    subprograms = future.result()
                    if subprograms:
                        self.programs_collected.extend(subprograms)
                        extracted_count += len(subprograms)
                except Exception as e:
                    pass
                
                current_done += 1
                if current_done % 1 == 0 or current_done == total:
                    print(f"   ... è¿›åº¦: {current_done}/{total} å¤§ç±», å·²è·å– {extracted_count} ä¸ªå­é¡¹ç›®", end='\r')
        
        print("") 
        log(f"\n   âœ… æå–å®Œæˆï¼Œå…±è·å– {len(self.programs_collected)} ä¸ªå­é¡¹ç›®")

    def _process_single_category(self, category_info: Dict) -> List[Dict]:
        """
        åœ¨ç‹¬ç«‹æµè§ˆå™¨ä¸­å¤„ç†å•ä¸ªå¤§ç±»
        """
        final_results = []
        name = category_info['name']
        cat_url = category_info.get('url')
        
        with self.browser_pool.get_browser() as browser:
            try:
                # --- Step 1: Open Detail Page ---
                # è®¾ç½®è¾ƒçŸ­è¶…æ—¶ï¼Œé˜²æ­¢å¡æ­»
                browser.set_page_load_timeout(20)
                try:
                    if cat_url and "http" in cat_url:
                        browser.get(cat_url)
                    else:
                        # Fallback
                        slug = re.sub(r'[^a-z0-9\s-]', '', name.lower()).strip().replace(' ', '-')
                        fallback_url = f"https://www.harvard.edu/programs/{slug}/"
                        browser.get(fallback_url)
                except TimeoutException:
                    pass # å¿½ç•¥åŠ è½½è¶…æ—¶ï¼Œåªè¦ DOM ç¨å¾®åŠ è½½å‡ºæ¥å°±è¡Œ
                
                # æ¢å¤é»˜è®¤è¶…æ—¶
                browser.set_page_load_timeout(60)
                
                WebDriverWait(browser, 20).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "body"))
                )

                # --- Step 2: Find "Graduate" Accordion ---
                basic_infos = []
                
                try:
                    # ç­‰å¾…ä¸€ä¸‹ accordion åŠ è½½
                    WebDriverWait(browser, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".c-accordion__header"))
                    )
                    
                    headers = browser.find_elements(By.CSS_SELECTOR, ".c-accordion__header")
                    grad_header = None
                    for h in headers:
                        t = h.text.strip().lower()
                        # Strict match or ensure it starts with Graduate and not Undergraduate
                        if t == "graduate" or (t.startswith("graduate") and "undergraduate" not in t):
                            grad_header = h
                            break
                    
                    if grad_header:
                        # Expand if needed
                        is_expanded = grad_header.get_attribute("aria-expanded")
                        if is_expanded != "true":
                            safe_click(browser, grad_header)
                            time.sleep(1) # wait animation
                        
                        # Find content area
                        # é€šå¸¸æ˜¯ç´§é‚»çš„ sibling div with class c-accordion__content
                        try:
                            content_area = grad_header.find_element(By.XPATH, "following-sibling::div[contains(@class, 'c-accordion__content')]")
                        except:
                            # å°è¯•é€šè¿‡ aria-controls
                            controls_id = grad_header.get_attribute("aria-controls")
                            if controls_id:
                                content_area = browser.find_element(By.ID, controls_id)
                            else:
                                content_area = None
                        
                        if content_area:
                            basic_infos = self._extract_subprograms_from_content(content_area, name, browser)
                            
                except TimeoutException:
                    pass
                except Exception:
                    pass
                
                # --- Step 3: Visit Detail Pages for Deadline ---
                for program_name, school, url in basic_infos:
                    deadline = "N/A"
                    if url and url != "N/A" and url.startswith("http"):
                        try:
                            browser.get(url)
                            deadline = self._extract_deadline_from_page(browser)
                        except Exception:
                            deadline = "Error Fetching"
                    
                    # Construct Final Result
                    result = self.create_result_template(program_name, url)
                    result["å­¦é™¢/å­¦ä¹ é¢†åŸŸ"] = school
                    result["é¡¹ç›®deadline"] = deadline
                    
                    # Add hardcoded application link
                    result["ç”³è¯·é“¾æ¥"] = "https://apply.gsas.harvard.edu/account/register?r=/portal/apply_degree"
                    
                    final_results.append(result)

            except Exception as e:
                pass
                
        return final_results

    def _extract_subprograms_from_content(self, content_area, category_name, browser) -> List[Tuple[str, str, str]]:
        """ä»å±•å¼€çš„å†…å®¹åŒºåŸŸæå–å­é¡¹ç›®"""
        extracted = []
        try:
            # æŸ¥æ‰¾æ‰€æœ‰å­é¡¹ç›®å—
            blocks = content_area.find_elements(By.CSS_SELECTOR, ".c-programs-accordion-content__degree")
            if not blocks:
                blocks = content_area.find_elements(By.CSS_SELECTOR, ".c-programs-accordion-content__program")
            if not blocks:
                blocks = content_area.find_elements(By.CSS_SELECTOR, "div.c-programs-accordion-content > div")

            for block in blocks:
                try:
                    # Title
                    degree_title = ""
                    try:
                        title_el = block.find_element(By.CSS_SELECTOR, ".c-programs-accordion-content__degree__title")
                        degree_title = title_el.text.strip()
                    except:
                        try:
                            degree_title = block.find_element(By.TAG_NAME, "h3").text.strip()
                        except:
                            continue 

                    # School
                    school = "N/A"
                    try:
                        school_el = block.find_element(By.CSS_SELECTOR, ".c-programs-accordion-content__degree__subtitle")
                        school = school_el.text.strip()
                    except:
                        pass
                        
                    # Learn More URL
                    learn_more_url = "N/A"
                    try:
                        potential_links = []
                        
                        # 1. Check sibling container (Most likely structure: degree + links are siblings)
                        try:
                            # Use a broad check for any sibling with 'links' in class, or just the next sibling
                            # Using relative xpath to find the links container associated with this degree header
                            # Assuming standard structure: degree -> description -> links
                            # So looking for following-sibling::div[contains(@class, '__links')]
                            links_container = block.find_element(By.XPATH, "following-sibling::div[contains(@class, '__links')]")
                            potential_links.extend(links_container.find_elements(By.TAG_NAME, "a"))
                        except:
                            pass

                        # 2. Check inside the block (Fallback if structure is nested)
                        potential_links.extend(block.find_elements(By.TAG_NAME, "a"))
                        
                        # Process all candidates
                        for link in potential_links:
                            # Check aria-label
                            aria = link.get_attribute("aria-label") or ""
                            # Check text/innerText
                            txt = link.text or link.get_attribute("innerText") or ""
                            txt = txt.strip().lower()
                            aria = aria.lower()
                            
                            if "learn more" in txt or "visit program" in txt or "learn more" in aria:
                                learn_more_url = link.get_attribute("href")
                                break
                        
                        if learn_more_url != "N/A" and not learn_more_url.startswith('http'):
                             learn_more_url = urljoin("https://www.harvard.edu", learn_more_url)
                             
                    except Exception as e:
                        pass
                    
                    full_name = f"{category_name} - {degree_title}"
                    extracted.append((full_name, school, learn_more_url))
                    
                except Exception:
                    continue
        except Exception:
            pass
        return extracted


    def _extract_deadline_from_page(self, browser: WebDriver) -> str:
        """ä»è¯¦æƒ…é¡µæå– Deadline"""
        try:
            WebDriverWait(browser, 5).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(1)
            
            page_text = browser.find_element(By.TAG_NAME, "body").text
            lines = [l.strip() for l in page_text.split('\n') if l.strip()]

            # 1. GSAS Specific "APPLICATION DEADLINE" header
            for i, line in enumerate(lines):
                if "APPLICATION DEADLINE" in line.upper():
                    # Check next 3 lines for a date
                    for j in range(1, 4):
                        if i + j < len(lines):
                            candidate = lines[i+j]
                            # Match months (Dec, January, etc) and year 202X
                            if re.search(r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s+\d{1,2},?\s+202\d', candidate, re.IGNORECASE):
                                return candidate
                            # Also check simple "December 15, 2025" without abbreviated month if different
                    
            # 2. Field Label Exact match
            try:
                labels = browser.find_elements(By.CSS_SELECTOR, ".field-label")
                for lbl in labels:
                    if "deadline" in lbl.text.lower():
                        try:
                            parent = lbl.find_element(By.XPATH, "..")
                            value_div = parent.find_element(By.CSS_SELECTOR, ".field__item")
                            val = value_div.text.strip()
                            if val: return val
                        except:
                            pass
            except:
                pass

            # 3. GSD Specific (Important Dates / Calendar Accordion)
            try:
                # Look for the container that has "deadline" in the title
                # Structure: .calendar-accordion__toggle > .calendar-accordion__title (text "PhD application deadline")
                # Sibling: .calendar-accordion__date > .calendar-accordion__calendar (text "January 5, 2026")
                
                toggles = browser.find_elements(By.CSS_SELECTOR, ".calendar-accordion__toggle")
                for toggle in toggles:
                    try:
                        title = toggle.find_element(By.CSS_SELECTOR, ".calendar-accordion__title").text.lower()
                        if "deadline" in title:
                            # Extract date
                            date_div = toggle.find_element(By.CSS_SELECTOR, ".calendar-accordion__date")
                            # It might have multiple spans, join them
                            spans = date_div.find_elements(By.CSS_SELECTOR, ".calendar-accordion__calendar")
                            texts = [s.text.strip() for s in spans if s.text.strip()]
                            if texts:
                                return " ".join(texts)
                    except:
                        continue
            except:
                pass

            # 4. Fallback Keyword Search
            for line in lines:
                lower = line.lower()
                clean_line = line.strip()
                # Must contain "deadline" or similar AND a year 202X
                if (("application due" in lower) or ("apply by" in lower) or ("deadline" in lower)) and re.search(r'202[4-6]', lower):
                     if len(clean_line) < 150:
                        return clean_line

            return "N/A"
        except:
            return "N/A"


    def _navigate_with_retry(self, max_retries: int = 3):
        pass

if __name__ == "__main__":
    HARVARD_MAX_WORKERS = 24
    with HarvardSpider(headless=True) as spider:
        results = spider.run()
        print(f"\næŠ“å–å®Œæˆï¼Œå…± {len(results)} ä¸ªé¡¹ç›®")
